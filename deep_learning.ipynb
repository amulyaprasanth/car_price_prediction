{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the deep learning models fo the car price prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers, Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_name</th>\n",
       "      <th>vehicle_age</th>\n",
       "      <th>km_driven</th>\n",
       "      <th>seller_type</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>mileage</th>\n",
       "      <th>engine</th>\n",
       "      <th>max_power</th>\n",
       "      <th>seats</th>\n",
       "      <th>selling_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maruti Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>120000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>19.70</td>\n",
       "      <td>796</td>\n",
       "      <td>46.30</td>\n",
       "      <td>5</td>\n",
       "      <td>120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hyundai Grand</td>\n",
       "      <td>5</td>\n",
       "      <td>20000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1197</td>\n",
       "      <td>82.00</td>\n",
       "      <td>5</td>\n",
       "      <td>550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hyundai i20</td>\n",
       "      <td>11</td>\n",
       "      <td>60000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1197</td>\n",
       "      <td>80.00</td>\n",
       "      <td>5</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maruti Alto</td>\n",
       "      <td>9</td>\n",
       "      <td>37000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>20.92</td>\n",
       "      <td>998</td>\n",
       "      <td>67.10</td>\n",
       "      <td>5</td>\n",
       "      <td>226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ford Ecosport</td>\n",
       "      <td>6</td>\n",
       "      <td>30000</td>\n",
       "      <td>Dealer</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>22.77</td>\n",
       "      <td>1498</td>\n",
       "      <td>98.59</td>\n",
       "      <td>5</td>\n",
       "      <td>570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        car_name  vehicle_age  km_driven seller_type fuel_type  \\\n",
       "0    Maruti Alto            9     120000  Individual    Petrol   \n",
       "1  Hyundai Grand            5      20000  Individual    Petrol   \n",
       "2    Hyundai i20           11      60000  Individual    Petrol   \n",
       "3    Maruti Alto            9      37000  Individual    Petrol   \n",
       "4  Ford Ecosport            6      30000      Dealer    Diesel   \n",
       "\n",
       "  transmission_type  mileage  engine  max_power  seats  selling_price  \n",
       "0            Manual    19.70     796      46.30      5         120000  \n",
       "1            Manual    18.90    1197      82.00      5         550000  \n",
       "2            Manual    17.00    1197      80.00      5         215000  \n",
       "3            Manual    20.92     998      67.10      5         226000  \n",
       "4            Manual    22.77    1498      98.59      5         570000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the preprocessed data\n",
    "data = pd.read_csv(\"preprocessed_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the car_name column before splitting to avoid not seen label error \n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "data[\"car_name\"]=data[\"car_name\"].apply(str.lower)\n",
    "\n",
    "data[\"car_name\"] = encoder.fit_transform(data[\"car_name\"])\n",
    "\n",
    "np.save('./app/car_name.npy', encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12328, 12328, 3083, 3083)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into features and labels\n",
    "features = data.drop(\"selling_price\", axis=1)\n",
    "labels = data[\"selling_price\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "len(X_train), len(y_train), len(X_val), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert non-numerical columns to numerical columns\n",
    "\n",
    "cat_columns = [\"seller_type\", \"fuel_type\", \"transmission_type\"]\n",
    "\n",
    "for label in cat_columns:\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    # X_train[label]=X_train[label].apply(str.lower)\n",
    "    # X_val[label]=X_val[label].apply(str.lower)\n",
    "        \n",
    "    X_train[label] = encoder.fit_transform(X_train[label])\n",
    "    X_val[label] = encoder.transform(X_val[label])\n",
    "    np.save(f'./app/{label}.npy', encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to array\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the dataset\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshaping the labels to avoid shape error when fitting scaler\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_val = y_val.to_numpy().reshape(-1, 1)\n",
    "\n",
    "scaler.fit(y_train)\n",
    "\n",
    "# Scaling\n",
    "# y_train = scaler.transform(y_train)\n",
    "# y_val = scaler.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                352       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,025\n",
      "Trainable params: 1,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train[0].shape)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "# compile model\n",
    "model.compile(loss='mae', optimizer=optimizers.Adam(learning_rate=2e-3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "386/386 [==============================] - 2s 3ms/step - loss: 584601.9375 - val_loss: 536436.8125\n",
      "Epoch 2/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 470709.0312 - val_loss: 415930.5938\n",
      "Epoch 3/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 309645.9688 - val_loss: 306678.3125\n",
      "Epoch 4/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 288983.1875 - val_loss: 303500.2812\n",
      "Epoch 5/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 288006.5938 - val_loss: 302269.1875\n",
      "Epoch 6/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 286530.1250 - val_loss: 301643.6562\n",
      "Epoch 7/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 285333.6250 - val_loss: 300564.3750\n",
      "Epoch 8/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 284126.5938 - val_loss: 301769.4688\n",
      "Epoch 9/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 283186.0000 - val_loss: 300753.3438\n",
      "Epoch 10/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 282567.0312 - val_loss: 298713.6875\n",
      "Epoch 11/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 281433.7812 - val_loss: 296712.0312\n",
      "Epoch 12/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 280448.3438 - val_loss: 295507.6875\n",
      "Epoch 13/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 279574.1562 - val_loss: 299052.9375\n",
      "Epoch 14/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 278605.7188 - val_loss: 294660.6562\n",
      "Epoch 15/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 277731.7500 - val_loss: 293022.5938\n",
      "Epoch 16/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 276746.8125 - val_loss: 293273.3750\n",
      "Epoch 17/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 276109.5000 - val_loss: 291955.4062\n",
      "Epoch 18/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 274547.0938 - val_loss: 292351.0312\n",
      "Epoch 19/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 273719.0000 - val_loss: 289443.4688\n",
      "Epoch 20/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 272713.2188 - val_loss: 287322.9375\n",
      "Epoch 21/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 271290.6562 - val_loss: 288277.4375\n",
      "Epoch 22/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 270473.5000 - val_loss: 285721.9375\n",
      "Epoch 23/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 269202.8750 - val_loss: 287688.3125\n",
      "Epoch 24/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 267658.7500 - val_loss: 285690.9375\n",
      "Epoch 25/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 266258.0625 - val_loss: 282353.2812\n",
      "Epoch 26/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 265113.6562 - val_loss: 281275.3750\n",
      "Epoch 27/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 262871.7812 - val_loss: 277807.7812\n",
      "Epoch 28/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 262001.2344 - val_loss: 277961.5312\n",
      "Epoch 29/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 259594.8750 - val_loss: 273496.6562\n",
      "Epoch 30/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 257829.6250 - val_loss: 271481.2812\n",
      "Epoch 31/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 255441.6562 - val_loss: 269705.2500\n",
      "Epoch 32/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 252814.5625 - val_loss: 266562.5938\n",
      "Epoch 33/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 251305.0938 - val_loss: 264094.5312\n",
      "Epoch 34/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 248459.5469 - val_loss: 263983.8438\n",
      "Epoch 35/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 244905.6562 - val_loss: 261094.6562\n",
      "Epoch 36/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 242565.7344 - val_loss: 256097.2500\n",
      "Epoch 37/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 240068.1094 - val_loss: 250721.9375\n",
      "Epoch 38/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 236225.1875 - val_loss: 245632.3906\n",
      "Epoch 39/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 231819.3594 - val_loss: 243590.0156\n",
      "Epoch 40/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 227464.6875 - val_loss: 238910.2344\n",
      "Epoch 41/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 222487.9062 - val_loss: 233708.7500\n",
      "Epoch 42/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 220334.5938 - val_loss: 242633.6562\n",
      "Epoch 43/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 216744.8906 - val_loss: 228947.3125\n",
      "Epoch 44/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 211351.9844 - val_loss: 227209.4062\n",
      "Epoch 45/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 209672.3281 - val_loss: 217067.7188\n",
      "Epoch 46/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 206452.3125 - val_loss: 234137.2500\n",
      "Epoch 47/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 203840.1875 - val_loss: 213588.1094\n",
      "Epoch 48/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 201421.2031 - val_loss: 209666.6406\n",
      "Epoch 49/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 200716.0469 - val_loss: 214263.7812\n",
      "Epoch 50/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 198482.6094 - val_loss: 222556.7031\n",
      "Epoch 51/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 196491.1875 - val_loss: 205405.7969\n",
      "Epoch 52/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 197238.7344 - val_loss: 209026.7656\n",
      "Epoch 53/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 195652.3438 - val_loss: 212294.5938\n",
      "Epoch 54/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 194611.5469 - val_loss: 202941.4688\n",
      "Epoch 55/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 193067.2344 - val_loss: 202286.5000\n",
      "Epoch 56/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 191017.9219 - val_loss: 202126.4219\n",
      "Epoch 57/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 191290.2656 - val_loss: 203040.9062\n",
      "Epoch 58/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 191050.9219 - val_loss: 204338.5312\n",
      "Epoch 59/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 189899.2344 - val_loss: 200180.3125\n",
      "Epoch 60/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 191082.8125 - val_loss: 202337.0625\n",
      "Epoch 61/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 189883.0000 - val_loss: 196558.7656\n",
      "Epoch 62/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 190323.9219 - val_loss: 196696.9531\n",
      "Epoch 63/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 187757.6562 - val_loss: 203202.1250\n",
      "Epoch 64/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 187244.7969 - val_loss: 203198.7500\n",
      "Epoch 65/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 187722.0469 - val_loss: 200855.4219\n",
      "Epoch 66/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 187088.0469 - val_loss: 200193.1875\n",
      "Epoch 67/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 187603.4219 - val_loss: 196655.2656\n",
      "Epoch 68/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 185806.4375 - val_loss: 195580.2500\n",
      "Epoch 69/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 187109.1875 - val_loss: 199778.1562\n",
      "Epoch 70/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 185187.8906 - val_loss: 196394.0625\n",
      "Epoch 71/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 184448.6875 - val_loss: 218162.2969\n",
      "Epoch 72/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 184277.2656 - val_loss: 191506.3281\n",
      "Epoch 73/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 184809.7812 - val_loss: 193358.1406\n",
      "Epoch 74/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 184249.7344 - val_loss: 193962.0000\n",
      "Epoch 75/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 182278.8125 - val_loss: 200514.8438\n",
      "Epoch 76/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 182925.2656 - val_loss: 217400.0938\n",
      "Epoch 77/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 182155.0469 - val_loss: 191604.3281\n",
      "Epoch 78/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 183376.8594 - val_loss: 191874.6094\n",
      "Epoch 79/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 181628.2188 - val_loss: 188804.6562\n",
      "Epoch 80/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 181117.8594 - val_loss: 193961.2344\n",
      "Epoch 81/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 180434.5781 - val_loss: 201120.6406\n",
      "Epoch 82/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 181745.3125 - val_loss: 189492.1094\n",
      "Epoch 83/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 182040.5938 - val_loss: 188072.1875\n",
      "Epoch 84/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 179560.7188 - val_loss: 197193.7344\n",
      "Epoch 85/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 180336.4531 - val_loss: 188461.5469\n",
      "Epoch 86/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 179770.2969 - val_loss: 186585.9531\n",
      "Epoch 87/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 181686.7969 - val_loss: 189313.7500\n",
      "Epoch 88/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 177802.0781 - val_loss: 188451.3750\n",
      "Epoch 89/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 181893.4375 - val_loss: 192930.5625\n",
      "Epoch 90/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 178836.2500 - val_loss: 185543.5625\n",
      "Epoch 91/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 178275.2656 - val_loss: 201191.6719\n",
      "Epoch 92/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 181201.8594 - val_loss: 184695.1562\n",
      "Epoch 93/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 178373.6406 - val_loss: 192589.3125\n",
      "Epoch 94/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 177624.2344 - val_loss: 185746.1250\n",
      "Epoch 95/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 179690.3281 - val_loss: 192411.8125\n",
      "Epoch 96/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 177844.9531 - val_loss: 186324.5156\n",
      "Epoch 97/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 176901.5312 - val_loss: 183784.3750\n",
      "Epoch 98/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 177866.4062 - val_loss: 184572.5312\n",
      "Epoch 99/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 175813.5938 - val_loss: 189413.9062\n",
      "Epoch 100/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 175769.7500 - val_loss: 184254.8594\n",
      "Epoch 101/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 178856.1406 - val_loss: 208609.7969\n",
      "Epoch 102/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 176355.5469 - val_loss: 184529.2031\n",
      "Epoch 103/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 176650.6719 - val_loss: 189592.5000\n",
      "Epoch 104/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 175250.1250 - val_loss: 186036.6250\n",
      "Epoch 105/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 175660.9219 - val_loss: 183147.0312\n",
      "Epoch 106/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 176197.9844 - val_loss: 191899.3594\n",
      "Epoch 107/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 175405.3281 - val_loss: 196967.4219\n",
      "Epoch 108/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 175481.7500 - val_loss: 185089.2188\n",
      "Epoch 109/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 174458.6250 - val_loss: 207149.6562\n",
      "Epoch 110/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 175503.5938 - val_loss: 181924.4688\n",
      "Epoch 111/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 174875.3750 - val_loss: 182688.8125\n",
      "Epoch 112/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 173155.0000 - val_loss: 181390.0156\n",
      "Epoch 113/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 174490.0781 - val_loss: 181387.4375\n",
      "Epoch 114/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 173613.6875 - val_loss: 205136.7969\n",
      "Epoch 115/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 173687.2031 - val_loss: 191499.8594\n",
      "Epoch 116/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 173272.7969 - val_loss: 182686.8750\n",
      "Epoch 117/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 173396.2656 - val_loss: 190444.1719\n",
      "Epoch 118/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 172459.9375 - val_loss: 183081.2969\n",
      "Epoch 119/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 175350.3281 - val_loss: 183084.4219\n",
      "Epoch 120/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 173353.9375 - val_loss: 185381.8281\n",
      "Epoch 121/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 174354.7969 - val_loss: 186145.7188\n",
      "Epoch 122/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 172085.4844 - val_loss: 180285.5469\n",
      "Epoch 123/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 171450.4844 - val_loss: 185798.8438\n",
      "Epoch 124/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 171163.5312 - val_loss: 188301.2500\n",
      "Epoch 125/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 173293.6562 - val_loss: 213319.6875\n",
      "Epoch 126/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 172264.0469 - val_loss: 185537.6562\n",
      "Epoch 127/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 171226.3594 - val_loss: 184736.4219\n",
      "Epoch 128/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 172053.5000 - val_loss: 181011.7031\n",
      "Epoch 129/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 171364.5312 - val_loss: 178781.6875\n",
      "Epoch 130/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 171496.5625 - val_loss: 179809.4219\n",
      "Epoch 131/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 173286.0625 - val_loss: 177149.5312\n",
      "Epoch 132/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 170388.8281 - val_loss: 180387.1250\n",
      "Epoch 133/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 170111.2500 - val_loss: 178412.5625\n",
      "Epoch 134/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 170568.2812 - val_loss: 181870.3906\n",
      "Epoch 135/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 168720.7344 - val_loss: 190658.4688\n",
      "Epoch 136/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 170144.5469 - val_loss: 180490.4219\n",
      "Epoch 137/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 171812.0000 - val_loss: 179207.8750\n",
      "Epoch 138/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 168860.3281 - val_loss: 194915.8281\n",
      "Epoch 139/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 169718.9219 - val_loss: 198127.6562\n",
      "Epoch 140/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 169206.2812 - val_loss: 181009.4531\n",
      "Epoch 141/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 169122.1250 - val_loss: 187986.5781\n",
      "Epoch 142/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 172175.4062 - val_loss: 181719.3750\n",
      "Epoch 143/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 169155.7500 - val_loss: 179073.0781\n",
      "Epoch 144/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 168247.8438 - val_loss: 177007.8281\n",
      "Epoch 145/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 170231.7344 - val_loss: 199557.8906\n",
      "Epoch 146/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 170621.0469 - val_loss: 185413.1719\n",
      "Epoch 147/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 166995.8750 - val_loss: 200959.6719\n",
      "Epoch 148/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 168292.1562 - val_loss: 201428.2656\n",
      "Epoch 149/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 167554.4219 - val_loss: 181654.5469\n",
      "Epoch 150/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 166756.0156 - val_loss: 183033.8906\n",
      "Epoch 151/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 168387.4375 - val_loss: 179233.6562\n",
      "Epoch 152/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 168331.2812 - val_loss: 175984.3906\n",
      "Epoch 153/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 166479.7031 - val_loss: 176249.1719\n",
      "Epoch 154/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 167968.7031 - val_loss: 191061.1719\n",
      "Epoch 155/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 168188.2188 - val_loss: 175803.6250\n",
      "Epoch 156/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 167560.9531 - val_loss: 178578.5312\n",
      "Epoch 157/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 167428.2500 - val_loss: 176601.9844\n",
      "Epoch 158/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 167472.8125 - val_loss: 176147.3125\n",
      "Epoch 159/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 165989.0938 - val_loss: 188613.9531\n",
      "Epoch 160/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 166651.4844 - val_loss: 188472.8438\n",
      "Epoch 161/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165308.7188 - val_loss: 173818.0781\n",
      "Epoch 162/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165934.9375 - val_loss: 201742.7344\n",
      "Epoch 163/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 168251.3125 - val_loss: 176420.6406\n",
      "Epoch 164/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164748.7344 - val_loss: 187356.5156\n",
      "Epoch 165/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 167152.1719 - val_loss: 176245.2500\n",
      "Epoch 166/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164722.7969 - val_loss: 186542.4844\n",
      "Epoch 167/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165424.7969 - val_loss: 177347.5312\n",
      "Epoch 168/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 166942.7812 - val_loss: 209962.9688\n",
      "Epoch 169/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 166142.4688 - val_loss: 175597.5312\n",
      "Epoch 170/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165027.5156 - val_loss: 175863.5469\n",
      "Epoch 171/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164194.0000 - val_loss: 176411.6875\n",
      "Epoch 172/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165937.3438 - val_loss: 190353.2656\n",
      "Epoch 173/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162859.8125 - val_loss: 183114.3438\n",
      "Epoch 174/1000\n",
      "386/386 [==============================] - 1s 3ms/step - loss: 166921.1406 - val_loss: 173416.8438\n",
      "Epoch 175/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165239.7656 - val_loss: 172941.2812\n",
      "Epoch 176/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165381.5469 - val_loss: 174169.1250\n",
      "Epoch 177/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163645.8906 - val_loss: 176391.3750\n",
      "Epoch 178/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 167796.5938 - val_loss: 172762.3906\n",
      "Epoch 179/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162949.6875 - val_loss: 183279.2656\n",
      "Epoch 180/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165861.7656 - val_loss: 179801.7812\n",
      "Epoch 181/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 165789.0156 - val_loss: 175002.9219\n",
      "Epoch 182/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164698.7969 - val_loss: 173263.2656\n",
      "Epoch 183/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164094.1094 - val_loss: 176861.1719\n",
      "Epoch 184/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164456.4844 - val_loss: 175941.4531\n",
      "Epoch 185/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 166072.0312 - val_loss: 179959.8594\n",
      "Epoch 186/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164580.4688 - val_loss: 174641.8438\n",
      "Epoch 187/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164683.1562 - val_loss: 173441.2969\n",
      "Epoch 188/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164881.5312 - val_loss: 187602.5938\n",
      "Epoch 189/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164771.2969 - val_loss: 183864.6094\n",
      "Epoch 190/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164481.7188 - val_loss: 176668.0312\n",
      "Epoch 191/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164064.1875 - val_loss: 170468.7812\n",
      "Epoch 192/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163203.2344 - val_loss: 173866.5469\n",
      "Epoch 193/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164327.1719 - val_loss: 172571.6250\n",
      "Epoch 194/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162906.5469 - val_loss: 174641.2656\n",
      "Epoch 195/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163095.9844 - val_loss: 173366.2500\n",
      "Epoch 196/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164229.5781 - val_loss: 174622.5625\n",
      "Epoch 197/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162856.1094 - val_loss: 176072.5469\n",
      "Epoch 198/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163993.9375 - val_loss: 178848.3906\n",
      "Epoch 199/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163991.3125 - val_loss: 174517.3438\n",
      "Epoch 200/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162880.1719 - val_loss: 185624.9688\n",
      "Epoch 201/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162622.7812 - val_loss: 169566.9375\n",
      "Epoch 202/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164277.1250 - val_loss: 174892.2812\n",
      "Epoch 203/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163374.1250 - val_loss: 179216.1094\n",
      "Epoch 204/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162968.0312 - val_loss: 172259.7812\n",
      "Epoch 205/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163469.2031 - val_loss: 183843.2969\n",
      "Epoch 206/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162000.0469 - val_loss: 184330.7188\n",
      "Epoch 207/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164734.0000 - val_loss: 184930.1562\n",
      "Epoch 208/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162138.4219 - val_loss: 181660.4844\n",
      "Epoch 209/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164583.2031 - val_loss: 183670.6094\n",
      "Epoch 210/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163393.1719 - val_loss: 174251.5469\n",
      "Epoch 211/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 166043.7812 - val_loss: 178684.6719\n",
      "Epoch 212/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164278.2344 - val_loss: 174384.7188\n",
      "Epoch 213/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162849.1562 - val_loss: 172223.5312\n",
      "Epoch 214/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163220.1250 - val_loss: 173847.6875\n",
      "Epoch 215/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164289.5156 - val_loss: 171999.4219\n",
      "Epoch 216/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 161618.5781 - val_loss: 182063.0781\n",
      "Epoch 217/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164580.7500 - val_loss: 179054.9219\n",
      "Epoch 218/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 162023.0156 - val_loss: 176170.3281\n",
      "Epoch 219/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 161275.1562 - val_loss: 200661.7812\n",
      "Epoch 220/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 164721.1562 - val_loss: 178283.6094\n",
      "Epoch 221/1000\n",
      "386/386 [==============================] - 1s 2ms/step - loss: 163368.8906 - val_loss: 186091.5781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f21fb69690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1000, validation_data=(X_val, y_val),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=20), tf.keras.callbacks.ModelCheckpoint(\"./app/checkpoints/model_checkpoint\", save_best_only=True, save_weights_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 1ms/step\n",
      "[[ 233223.47]\n",
      " [ 558349.5 ]\n",
      " [ 476534.5 ]\n",
      " [1692466.6 ]\n",
      " [ 122750.7 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.1003592e+11]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"./app/checkpoints/model_checkpoint\")\n",
    "preds = model.predict(X_val)\n",
    "print(preds[:5])\n",
    "scaler.inverse_transform(preds[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 190000]\n",
      " [ 600000]\n",
      " [ 665000]\n",
      " [1570000]\n",
      " [ 160000]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.71109965e+11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_val[:5])\n",
    "scaler.inverse_transform(y_val[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/97 [..............................] - ETA: 7sWARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "97/97 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 233223.47],\n",
       "       [ 558349.5 ],\n",
       "       [ 476534.5 ],\n",
       "       [1692466.6 ],\n",
       "       [ 122750.7 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.model import build_model\n",
    "\n",
    "loaded_model = build_model()\n",
    "loaded_model.load_weights('./app/checkpoints/model_checkpoint')\n",
    "\n",
    "loaded_model.predict(X_val)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[233223.47]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(tf.expand_dims(X_val[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is taken from geeks for geeks page (https://www.geeksforgeeks.org/convert-the-number-from-international-system-to-indian-system/)\n",
    "def convert(input):\n",
    "\n",
    "\t# Find the length of the\n",
    "\t# input string\n",
    "\tLen = len(input)\n",
    "\n",
    "\t# Removing all the separators(, )\n",
    "\t# from the input string\n",
    "\ti = 0\n",
    "\twhile(i < Len):\n",
    "\t\tif(input[i] == \",\"):\n",
    "\t\t\tinput = input[:i] + input[i + 1:]\n",
    "\t\t\tLen -= 1\n",
    "\t\t\ti -= 1\n",
    "\t\telif(input[i] == \" \"):\n",
    "\t\t\tinput=input[:i] + input[i + 1:]\n",
    "\t\t\tLen -= 1\n",
    "\t\t\ti -= 1\n",
    "\t\telse:\n",
    "\t\t\ti += 1\n",
    "\t# Reverse the input string\n",
    "\tinput=input[::-1]\n",
    "\n",
    "\t# Declaring the output string\n",
    "\toutput = \"\"\n",
    "\n",
    "\t# Process the input string\n",
    "\tfor i in range(Len):\n",
    "\n",
    "\t\t# Add a separator(, ) after the\n",
    "\t\t# third number\n",
    "\t\tif(i == 2):\n",
    "\t\t\toutput += input[i]\n",
    "\t\t\toutput += \",\"\n",
    "\t\t\n",
    "\t\t# Then add a separator(, ) after\n",
    "\t\t# every second number\n",
    "\t\telif(i > 2 and i % 2 == 0 and\n",
    "\t\t\ti + 1 < Len):\n",
    "\t\t\toutput += input[i]\n",
    "\t\t\toutput += \",\"\n",
    "\t\telse:\n",
    "\t\t\toutput += input[i]\n",
    "\t\n",
    "\t# Reverse the output string\n",
    "\toutput=output[::-1]\n",
    "\n",
    "\t# Return the output string back\n",
    "\t# to the main function\n",
    "\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "2,33,22,3.0\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.round(model.predict(tf.expand_dims(X_val[0], axis=0)))\n",
    "print(convert(str(prediction.numpy()[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.15.0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "import gradio\n",
    "\n",
    "\n",
    "gradio.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
